{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python392jvsc74a57bd0d9019ccc628dbb86587306b2b68b3479d266be4699362a26b08fd420b8d79845",
   "display_name": "Python 3.9.2 64-bit"
  },
  "metadata": {
   "interpreter": {
    "hash": "d9019ccc628dbb86587306b2b68b3479d266be4699362a26b08fd420b8d79845"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data_file(fname):\n",
    "    # Load dataset initially with all values as tring objects and skip first three rows under the header row\n",
    "    dtypes = {'TBLID': 'string', 'GEOID': 'string', 'GEONAME': 'string', 'PROFLN': 'float64', 'TITLE': 'string', 'ESTIMATE_1': 'string', 'MG_ERROR_1': 'string',\n",
    "    'ESTIMATE_2': 'string', 'MG_ERROR_2': 'string', 'ESTIMATE_3': 'string', 'MG_ERROR_3': 'string', 'ESTIMATE_4': 'string', 'MG_ERROR_4': 'string', 'ESTIMATE_5': 'string',\n",
    "    'MG_ERROR_5': 'string', 'ESTIMATE_6': 'string', 'MG_ERROR_6': 'string', 'ESTIMATE_7': 'string', 'MG_ERROR_7': 'string', 'ESTIMATE_8': 'string', 'MG_ERROR_8': 'string',\n",
    "    'OCC_CODE': 'string', 'OCC_DESCRIPTION': 'string'}\n",
    "    df = pd.read_csv(fname, skiprows=lambda r: r in [1,2,3], dtype=dtypes)\n",
    "\n",
    "    # Drop TBLID as it is of no use here\n",
    "    df.drop(columns=['TBLID'], inplace=True)\n",
    "\n",
    "    # Exclude rows where Title is not 'Number' or 'Percent' as there is no values in the EST and MOE fields for such rows\n",
    "    df_new = df[ (df.TITLE == 'Number') | (df.TITLE == 'Percent') ]\n",
    "\n",
    "    # Unpivot DataFrame from wide to long format leaving geography, occupation, profile number, and title as the identifier set\n",
    "    df_new_melted = df_new.melt(id_vars=['GEOID', 'GEONAME', 'OCC_CODE', 'OCC_DESCRIPTION', 'PROFLN', 'TITLE'], var_name='ATTRIBUTE', value_name='VALUE')\n",
    "\n",
    "    # Caputre report type in a column for use in attribute code\n",
    "    df_new_melted['REPORT_TYPE'] = ['EST' if 'EST' in rt else 'MOE' for rt in df_new_melted.ATTRIBUTE]\n",
    "\n",
    "    # Add an attribute code columm (for use in relating to 2014_2018_EEOALL1R_Column_Headers)\n",
    "\n",
    "    attr_re_map = {'ESTIMATE_1': 7, 'MG_ERROR_1': 7, 'ESTIMATE_2': 1, 'MG_ERROR_2': 1, 'ESTIMATE_3': 6, 'MG_ERROR_3': 6, 'ESTIMATE_4': 4, 'MG_ERROR_4': 4, 'ESTIMATE_5': 2, 'MG_ERROR_5': 2, 'ESTIMATE_6': 3, 'MG_ERROR_6': 3, 'ESTIMATE_7': 5, 'MG_ERROR_7': 5, 'ESTIMATE_8': 0, 'MG_ERROR_8': 0}\n",
    "    # Define a function to produce attribute codes from df\n",
    "    def concat_attr_code(report_type, gender_num, val_format, attr):\n",
    "        gender_num_map = {1: 'B', 2: 'B', 3: 'M', 4: 'M', 5: 'F', 6: 'F'}\n",
    "        val_format_map = {'Number': 'N', 'Percent': 'P'}\n",
    "        attr_tuple = (report_type, gender_num_map[gender_num], val_format_map[val_format], str(attr_re_map[attr]))\n",
    "        attr_code = '_'.join(attr_tuple)\n",
    "        return attr_code\n",
    "    df_new_melted['ATTR_CODE'] = df_new_melted.apply(lambda row: concat_attr_code(row['REPORT_TYPE'], row['PROFLN'], row['TITLE'], row['ATTRIBUTE']), axis=1)\n",
    "\n",
    "    # Drop columns used for attribute code as they are no longer needed\n",
    "    df_new_melted.drop(columns=['PROFLN', 'TITLE', 'ATTRIBUTE', 'REPORT_TYPE'], inplace=True)\n",
    "\n",
    "    return df_new_melted\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "nation_processed_df = process_data_file('2014_2018_EEOALL1R_010_Nation.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_processed_df = process_data_file('2014_2018_EEOALL1R_040_State.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "cbsa_processed_df = process_data_file('2014_2018_EEOALL1R_310_CBSA.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "countySet_processed_df = process_data_file('2014_2018_EEOALL1R_902_CountySets.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "nation_processed_df.to_csv('2014_2018_EEOALL1R_010_Nation_processed.csv', index=False)\n",
    "state_processed_df.to_csv('2014_2018_EEOALL1R_040_State_processed.csv', index=False)\n",
    "cbsa_processed_df.to_csv('2014_2018_EEOALL1R_310_CBSA_processed.csv', index=False)\n",
    "countySet_processed_df.to_csv('2014_2018_EEOALL1R_902_CountySets_processed.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Nation dataset has 22,848 rows.\nState dataset has 1,188,096 rows.\nCore Based Statistical Area dataset has 12,337,920 rows.\nCounty Set dataset has 33,700,800 rows.\n"
     ]
    }
   ],
   "source": [
    "def print_geo_record_count(df, df_name):\n",
    "    print('{1} dataset has {0:,} rows.'.format(df.shape[0], df_name))\n",
    "    return\n",
    "print_geo_record_count(nation_processed_df, 'Nation')\n",
    "print_geo_record_count(state_processed_df, 'State')\n",
    "print_geo_record_count(cbsa_processed_df, 'Core Based Statistical Area')\n",
    "print_geo_record_count(countySet_processed_df, 'County Set')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}